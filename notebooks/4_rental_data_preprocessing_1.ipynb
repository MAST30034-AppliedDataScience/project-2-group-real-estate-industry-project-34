{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for preprocessing the rental data, it is split up into 2 notebooks to handle any memory issues, with the number of JSON files being processed also being split up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First check if data from an agency was scraped twice, and remove manually from one of the directories if that occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate agency IDs found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Directories\n",
    "json_dir = \"../data/landing/\"\n",
    "json_dir_new = \"../data/landing/new/\"\n",
    "\n",
    "# Function to extract agency IDs from filenames\n",
    "def extract_agency_id(filename):\n",
    "    match = re.search(r'listings_agency_(\\d+)', filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# List all JSON files in both directories\n",
    "json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "json_files_new = [f for f in os.listdir(json_dir_new) if f.endswith('.json')]\n",
    "\n",
    "# Extract agency IDs from filenames in both directories\n",
    "agency_ids_landing = set(filter(None, [extract_agency_id(f) for f in json_files]))\n",
    "agency_ids_new = set(filter(None, [extract_agency_id(f) for f in json_files_new]))\n",
    "\n",
    "# Find duplicates by checking for common agency IDs between the two directories\n",
    "duplicate_agency_ids = agency_ids_landing.intersection(agency_ids_new)\n",
    "\n",
    "# Output results\n",
    "if duplicate_agency_ids:\n",
    "    print(f\"Duplicate agency IDs found: {duplicate_agency_ids}\")\n",
    "else:\n",
    "    print(\"No duplicate agency IDs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, combine all the JSONs from the first directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 112 DataFrames from the old directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load a JSON file into a DataFrame\n",
    "def load_json_to_df(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Directory containing the JSON files (Old Directory)\n",
    "json_dir = \"../data/landing/\"\n",
    "\n",
    "# List all JSON files in the old directory\n",
    "json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "\n",
    "# Initialize list to store DataFrames for the old directory\n",
    "dfs_old = []\n",
    "\n",
    "# Loop through each JSON file in the old directory, load it, and append to dfs_old\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(json_dir, json_file)\n",
    "    df = load_json_to_df(file_path)\n",
    "    \n",
    "    # Check if the DataFrame is empty\n",
    "    if not df.empty:\n",
    "        dfs_old.append(df)\n",
    "    else:\n",
    "        print(f\"Skipped empty DataFrame for file: {json_file}\")\n",
    "\n",
    "# Concatenate all DataFrames from the old directory into a single DataFrame\n",
    "print(f\"Concatenating {len(dfs_old)} DataFrames from the old directory\")\n",
    "compiled_df = pd.concat(dfs_old, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective                                      583466\n",
      "propertyTypes                                  583420\n",
      "status                                         583466\n",
      "saleMode                                       583466\n",
      "channel                                        583466\n",
      "                                                ...  \n",
      "advertiserIdentifiers.conjunctionContactIds         6\n",
      "advertiserIdentifiers.conjunctionAgentIds           6\n",
      "saleDetails.tenderDetails.tenderEndDate            18\n",
      "saleDetails.tenantDetails.leaseEndDate              3\n",
      "devProjectId                                        6\n",
      "Length: 99, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(compiled_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively convert unhashable types (dict, list) to hashable types (tuple)\n",
    "def make_hashable(item):\n",
    "    if isinstance(item, dict):\n",
    "        return tuple((key, make_hashable(value)) for key, value in sorted(item.items()))\n",
    "    elif isinstance(item, list):\n",
    "        return tuple(make_hashable(i) for i in item)\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "pdf = compiled_df\n",
    "\n",
    "# Apply the recursive conversion to all elements in the DataFrame\n",
    "for col in pdf.columns:\n",
    "    pdf[col] = pdf[col].apply(make_hashable)\n",
    "\n",
    "# Now you can drop duplicates\n",
    "pdf_cleaned = pdf.drop_duplicates()\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(pdf_cleaned.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for only rows that are rental data and not sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = compiled_df[compiled_df['objective'] == 'rent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective                                      389071\n",
       "propertyTypes                                  389048\n",
       "status                                         389071\n",
       "saleMode                                       389071\n",
       "channel                                        389071\n",
       "                                                ...  \n",
       "advertiserIdentifiers.conjunctionContactIds         4\n",
       "advertiserIdentifiers.conjunctionAgentIds           4\n",
       "saleDetails.tenderDetails.tenderEndDate             0\n",
       "saleDetails.tenantDetails.leaseEndDate              0\n",
       "devProjectId                                        0\n",
       "Length: 99, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove the dollar sign and commas\n",
    "filtered_df['priceDetails.displayPrice'] = filtered_df['priceDetails.displayPrice'].str.replace('$', '', regex=False)  # Remove dollar sign\n",
    "filtered_df['priceDetails.displayPrice'] = filtered_df['priceDetails.displayPrice'].str.replace(',', '', regex=False)  # Remove commas\n",
    "\n",
    "# Step 2: Convert the cleaned strings to floats\n",
    "filtered_df['priceDetails.displayPrice'] = pd.to_numeric(filtered_df['priceDetails.displayPrice'], errors='coerce')  # Convert to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for listings above 200 AUD as those below were found to mostly be carparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective                                    152345\n",
       "propertyTypes                                152338\n",
       "status                                       152345\n",
       "saleMode                                     152345\n",
       "channel                                      152345\n",
       "                                              ...  \n",
       "advertiserIdentifiers.conjunctionAgentIds         0\n",
       "saleDetails.tenderDetails.tenderEndDate           0\n",
       "saleDetails.tenantDetails.leaseEndDate            0\n",
       "devProjectId                                      0\n",
       "year                                         152345\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = filtered_df[filtered_df['priceDetails.price'] >= 200]\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep listings with a reasonable amount of bedrooms and bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter out unrealistic numbers of bathrooms and bedrooms\n",
    "filtered_df = filtered_df[(filtered_df['bathrooms'] >= 1) & (filtered_df['bathrooms'] <= 10)]\n",
    "filtered_df = filtered_df[(filtered_df['bedrooms'] >= 1) & (filtered_df['bedrooms'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective                                    151173\n",
       "propertyTypes                                151166\n",
       "status                                       151173\n",
       "saleMode                                     151173\n",
       "channel                                      151173\n",
       "                                              ...  \n",
       "advertiserIdentifiers.conjunctionAgentIds         0\n",
       "saleDetails.tenderDetails.tenderEndDate           0\n",
       "saleDetails.tenantDetails.leaseEndDate            0\n",
       "devProjectId                                      0\n",
       "year                                         151173\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  number_of_listings\n",
      "0   2004                 757\n",
      "1   2005                7667\n",
      "2   2006                6475\n",
      "3   2007                6885\n",
      "4   2008                7839\n",
      "5   2009                5353\n",
      "6   2010                3311\n",
      "7   2011                4612\n",
      "8   2012                8694\n",
      "9   2013                8943\n",
      "10  2014                7623\n",
      "11  2015                7996\n",
      "12  2016                5492\n",
      "13  2017                1710\n",
      "14  2018                3536\n",
      "15  2019               11024\n",
      "16  2020               13583\n",
      "17  2021               10608\n",
      "18  2022                9959\n",
      "19  2023               10577\n",
      "20  2024                8529\n"
     ]
    }
   ],
   "source": [
    "df = filtered_df\n",
    "\n",
    "# Convert 'dateListed' to datetime\n",
    "df['dateListed'] = pd.to_datetime(df['dateListed'])\n",
    "\n",
    "# Extract year from 'dateListed'\n",
    "df['year'] = df['dateListed'].dt.year\n",
    "\n",
    "# Count number of listings per year\n",
    "listings_per_year = df.groupby('year').size().reset_index(name='number_of_listings')\n",
    "\n",
    "# Print results\n",
    "print(listings_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "filtered_df.to_parquet('../data/raw/filtered_rental_listings_1.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
