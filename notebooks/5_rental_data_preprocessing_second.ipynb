{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m json_file \u001b[38;5;129;01min\u001b[39;00m json_files:\n\u001b[1;32m     22\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(json_dir, json_file)\n\u001b[0;32m---> 23\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_json_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Check if the DataFrame is empty\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mload_json_to_df\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_to_df\u001b[39m(file_path):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 8\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mjson_normalize(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load a JSON file into a DataFrame\n",
    "def load_json_to_df(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Directory containing the JSON files (Old Directory)\n",
    "json_dir = \"../data/landing/\"\n",
    "\n",
    "# List all JSON files in the old directory\n",
    "json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "\n",
    "# Initialize list to store DataFrames for the old directory\n",
    "dfs_old = []\n",
    "\n",
    "# Loop through each JSON file in the old directory, load it, and append to dfs_old\n",
    "for json_file in json_files:\n",
    "    file_path = os.path.join(json_dir, json_file)\n",
    "    df = load_json_to_df(file_path)\n",
    "    \n",
    "    # Check if the DataFrame is empty\n",
    "    if not df.empty:\n",
    "        dfs_old.append(df)\n",
    "    else:\n",
    "        print(f\"Skipped empty DataFrame for file: {json_file}\")\n",
    "\n",
    "# Concatenate all DataFrames from the old directory into a single DataFrame\n",
    "print(f\"Concatenating {len(dfs_old)} DataFrames from the old directory\")\n",
    "df1 = pd.concat(dfs_old, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating 77 DataFrames from the new directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load a JSON file into a DataFrame\n",
    "def load_json_to_df(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return pd.json_normalize(data)\n",
    "\n",
    "# Directory containing the JSON files (New Directory)\n",
    "json_dir_new = \"../data/landing/new/\"\n",
    "\n",
    "# List all JSON files in the new directory\n",
    "json_files_new = [f for f in os.listdir(json_dir_new) if f.endswith('.json')]\n",
    "\n",
    "# Initialize list to store DataFrames for the new directory\n",
    "dfs_new = []\n",
    "\n",
    "# Loop through each JSON file in the new directory, load it, and append to dfs_new\n",
    "for json_file in json_files_new:\n",
    "    file_path = os.path.join(json_dir_new, json_file)\n",
    "    df = load_json_to_df(file_path)\n",
    "    \n",
    "    # Check if the DataFrame is empty\n",
    "    if not df.empty:\n",
    "        dfs_new.append(df)\n",
    "    else:\n",
    "        print(f\"Skipped empty DataFrame for file: {json_file}\")\n",
    "\n",
    "# Concatenate all DataFrames from the new directory into a single DataFrame\n",
    "print(f\"Concatenating {len(dfs_new)} DataFrames from the new directory\")\n",
    "compiled_df = pd.concat(dfs_new, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective                                              319378\n",
      "propertyTypes                                          319193\n",
      "status                                                 319378\n",
      "saleMode                                               319378\n",
      "channel                                                319378\n",
      "                                                        ...  \n",
      "saleDetails.tenantDetails.tenantInfoTermOfLeaseFrom        15\n",
      "saleDetails.tenantDetails.tenantInfoTermOfLeaseTo          15\n",
      "saleDetails.tenantDetails.leaseStartDate                    5\n",
      "saleDetails.tenantDetails.leaseEndDate                      4\n",
      "devProjectId                                                1\n",
      "Length: 97, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(compiled_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively convert unhashable types (dict, list) to hashable types (tuple)\n",
    "def make_hashable(item):\n",
    "    if isinstance(item, dict):\n",
    "        return tuple((key, make_hashable(value)) for key, value in sorted(item.items()))\n",
    "    elif isinstance(item, list):\n",
    "        return tuple(make_hashable(i) for i in item)\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "pdf = compiled_df\n",
    "\n",
    "# Apply the recursive conversion to all elements in the DataFrame\n",
    "for col in pdf.columns:\n",
    "    pdf[col] = pdf[col].apply(make_hashable)\n",
    "\n",
    "# Now you can drop duplicates\n",
    "pdf_cleaned = pdf.drop_duplicates()\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(pdf_cleaned.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = compiled_df[compiled_df['objective'] == 'rent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective                                              149196\n",
       "propertyTypes                                          149100\n",
       "status                                                 149196\n",
       "saleMode                                               149196\n",
       "channel                                                149196\n",
       "                                                        ...  \n",
       "saleDetails.tenantDetails.tenantInfoTermOfLeaseFrom         0\n",
       "saleDetails.tenantDetails.tenantInfoTermOfLeaseTo           0\n",
       "saleDetails.tenantDetails.leaseStartDate                    0\n",
       "saleDetails.tenantDetails.leaseEndDate                      0\n",
       "devProjectId                                                0\n",
       "Length: 97, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered for only apartments and removed carparks as they skew the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_filtered = filtered_df[filtered_df['priceDetails.price'] <= 200]\n",
    "\n",
    "# Select relevant columns for inspection\n",
    "columns_of_interest = ['priceDetails.price', 'propertyTypes', 'headline', 'description', 'addressParts.displayAddress', 'dateListed']\n",
    "\n",
    "# Display the relevant information for these listings\n",
    "print(price_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_df['priceDetails.price'], kde=False)  # Let seaborn choose the number of bins automatically\n",
    "# Set x-axis limits\n",
    "plt.xlim(-500, 1400)\n",
    "plt.title('Distribution of Prices')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove the dollar sign and commas\n",
    "filtered_df['priceDetails.displayPrice'] = filtered_df['priceDetails.displayPrice'].str.replace('$', '', regex=False)  # Remove dollar sign\n",
    "filtered_df['priceDetails.displayPrice'] = filtered_df['priceDetails.displayPrice'].str.replace(',', '', regex=False)  # Remove commas\n",
    "\n",
    "# Step 2: Convert the cleaned strings to floats\n",
    "filtered_df['priceDetails.displayPrice'] = pd.to_numeric(filtered_df['priceDetails.displayPrice'], errors='coerce')  # Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the 'priceDetails.price' column\n",
    "missing_values = filtered_df['priceDetails.displayPrice'].isna().sum()\n",
    "print(f\"Number of missing values: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  number_of_listings\n",
      "0   2004                  51\n",
      "1   2005                 877\n",
      "2   2006                1275\n",
      "3   2007                2065\n",
      "4   2008                2226\n",
      "5   2009                2815\n",
      "6   2010                3303\n",
      "7   2011                4565\n",
      "8   2012                7433\n",
      "9   2013                8272\n",
      "10  2014                6813\n",
      "11  2015                6735\n",
      "12  2016                7760\n",
      "13  2017                8511\n",
      "14  2018                8034\n",
      "15  2019               10151\n",
      "16  2020               10566\n",
      "17  2021                9705\n",
      "18  2022               11254\n",
      "19  2023               13118\n",
      "20  2024               10518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_379002/1973548176.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['dateListed'] = pd.to_datetime(df['dateListed'])\n",
      "/tmp/ipykernel_379002/1973548176.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['year'] = df['dateListed'].dt.year\n"
     ]
    }
   ],
   "source": [
    "df = filtered_df\n",
    "\n",
    "# Convert 'dateListed' to datetime\n",
    "df['dateListed'] = pd.to_datetime(df['dateListed'])\n",
    "\n",
    "# Extract year from 'dateListed'\n",
    "df['year'] = df['dateListed'].dt.year\n",
    "\n",
    "# Count number of listings per year\n",
    "listings_per_year = df.groupby('year').size().reset_index(name='number_of_listings')\n",
    "\n",
    "# Print results\n",
    "print(listings_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective                                              49282\n",
       "propertyTypes                                          49279\n",
       "status                                                 49282\n",
       "saleMode                                               49282\n",
       "channel                                                49282\n",
       "                                                       ...  \n",
       "saleDetails.tenantDetails.tenantInfoTermOfLeaseFrom        0\n",
       "saleDetails.tenantDetails.tenantInfoTermOfLeaseTo          0\n",
       "saleDetails.tenantDetails.leaseStartDate                   0\n",
       "saleDetails.tenantDetails.leaseEndDate                     0\n",
       "devProjectId                                               0\n",
       "Length: 97, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = filtered_df[filtered_df['priceDetails.price'] >= 200]\n",
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Filter out unrealistic numbers of bathrooms and bedrooms\n",
    "filtered_df = filtered_df[(filtered_df['bathrooms'] >= 1) & (filtered_df['bathrooms'] <= 10)]\n",
    "filtered_df = filtered_df[(filtered_df['bedrooms'] >= 1) & (filtered_df['bedrooms'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "objective                                      48230\n",
       "propertyTypes                                  48226\n",
       "status                                         48230\n",
       "saleMode                                       48230\n",
       "channel                                        48230\n",
       "                                               ...  \n",
       "saleDetails.tenantDetails.leaseStartDate           0\n",
       "saleDetails.tenantDetails.leaseEndDate             0\n",
       "advertiserIdentifiers.conjunctionContactIds        0\n",
       "advertiserIdentifiers.conjunctionAgentIds          0\n",
       "year                                           48230\n",
       "Length: 99, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year  number_of_listings\n",
      "0   2004                  20\n",
      "1   2005                 437\n",
      "2   2006                 787\n",
      "3   2007                1377\n",
      "4   2008                1618\n",
      "5   2009                1827\n",
      "6   2010                 780\n",
      "7   2011                1399\n",
      "8   2012                3269\n",
      "9   2013                3867\n",
      "10  2014                2472\n",
      "11  2015                2687\n",
      "12  2016                2630\n",
      "13  2017                1845\n",
      "14  2018                1297\n",
      "15  2019                5140\n",
      "16  2020                4001\n",
      "17  2021                2907\n",
      "18  2022                3278\n",
      "19  2023                3630\n",
      "20  2024                2962\n"
     ]
    }
   ],
   "source": [
    "df = filtered_df\n",
    "\n",
    "# Convert 'dateListed' to datetime\n",
    "df['dateListed'] = pd.to_datetime(df['dateListed'])\n",
    "\n",
    "# Extract year from 'dateListed'\n",
    "df['year'] = df['dateListed'].dt.year\n",
    "\n",
    "# Count number of listings per year\n",
    "listings_per_year = df.groupby('year').size().reset_index(name='number_of_listings')\n",
    "\n",
    "# Print results\n",
    "print(listings_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_parquet('../data/raw/filtered_rental_listings_new.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
